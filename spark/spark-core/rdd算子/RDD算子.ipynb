{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD算子\n",
    "初始化spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"local[6]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.216.1:4040\n",
       "SparkContext available as 'sc' (version = 2.1.1, master = local[6], app id = local-1559487109383)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@561d9114\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@561d9114\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.SparkContext = org.apache.spark.SparkContext@5ccc3144\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建RDD\n",
    "### sc.parallelize\n",
    "```scala\n",
    "  def parallelize[T: ClassTag](seq: Seq[T],numSlices: Int = defaultParallelism): RDD[T]\n",
    "```\n",
    "\n",
    "### sc.makeRDD\n",
    "```scala\n",
    "  /** Distribute a local Scala collection to form an RDD.\n",
    "   *\n",
    "   * This method is identical to `parallelize`.\n",
    "   */\n",
    "  def makeRDD[T: ClassTag](\n",
    "      seq: Seq[T],\n",
    "      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n",
    "    parallelize(seq, numSlices)\n",
    "  }\n",
    "```\n",
    "### textFile\n",
    "```scala\n",
    "  def textFile(path: String,minPartitions: Int = defaultMinPartitions): RDD[String]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Int = 6\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//查看该rdd的分区数量，默认是程序所分配的cpu core的数量，也可以在创建的时候指定\n",
    " rdd1.partitions.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Double] = ParallelCollectionRDD[1] at parallelize at <console>:27\r\n",
       "res4: Int = 3\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(Array(1,2,3.4),3)\n",
    "rdd1.partitions.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。\n",
    "\n",
    "### map(func)\n",
    "```scala\n",
    "def map[U: ClassTag](f: T => U): RDD[U]\n",
    "```\n",
    "返回一个新的RDD，该RDD由每一个输入的元素经过func函数转换后组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at <console>:27\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mapRdd = rdd.map(_ * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapRdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(func)\n",
    "```scala\n",
    "def filter(f: T => Boolean): RDD[T]\n",
    "```\n",
    "返回一个新的RDD，该RDD由每一个输入的元素经过func函数计算后返回为true的输入元素组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Array(\"xiaoming\",\"xiaojiang\",\"xiaohe\",\"dazhi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:27\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterRdd = rdd.filter(_.contains(\"xiao\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[String] = Array(xiaoming, xiaojiang, xiaohe)\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterRdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap(func)\n",
    "```scala\n",
    "def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U]\n",
    "```\n",
    "将函数应用于 RDD 中的每个元素，将返回的迭代器的所有内容构成新的 RDD。通常用来切分单词。类似于先map，然后再flatten。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatMapRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at flatMap at <console>:27\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val  flatMapRdd = rdd.flatMap(1 to _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatMapRdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct([numTasks]))\n",
    "```scala\n",
    "  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]\n",
    "\n",
    "  def distinct(): RDD[T] = withScope {\n",
    "    distinct(partitions.length)\n",
    "  }\n",
    "```\n",
    "去重。对源RDD进行去重后返回一个新的RDD. 默认情况下，只有n个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,1,5,2,9,6,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinctRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at distinct at <console>:27\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distinctRdd  = rdd.distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[Int] = Array(6, 1, 2, 9, 5)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinctRdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[Int] = Array(6, 2, 1, 9, 5)\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.distinct(2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这个函数的顺序和原来的顺序不一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union(otherRdd)\n",
    "```scala\n",
    "def union(other: RDD[T]): RDD[T]\n",
    "```\n",
    "求并集，注意类型要一致，生成一个包含两个 RDD 中所有元素的 RDD。**相同的元素会出现多次**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at <console>:25\r\n",
       "rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 5)\n",
    "val rdd2 = sc.parallelize(3 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[Int] = Array(1, 2, 3, 4, 5, 3, 4, 5, 6, 7)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.union(rdd2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection(otherRdd)\n",
    "```scala\n",
    "def intersection(other: RDD[T]): RDD[T]\n",
    "\n",
    "def intersection(other: RDD[T],partitioner: Partitioner(implicit ord: Ordering[T] = null): RDD[T]\n",
    "\n",
    "def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope {\n",
    "  intersection(other, new HashPartitioner(numPartitions))\n",
    "}                \n",
    "```\n",
    "求交集，求两个 RDD 共同的元素的 RDD。**输出将不包含任何重复项**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[Int] = Array(3, 4, 5)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.intersection(rdd2).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[Int] = Array(1, 2, 3, 4, 5)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.union(rdd2).intersection(rdd1).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtract(otherRdd)\n",
    "```scala\n",
    "def subtract(other: RDD[T]): RDD[T]               \n",
    "```\n",
    "移除一个 RDD 中的内容（例如移除训练数据）。**输出可以包含任何重复项**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[36] at subtract at <console>:25\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val subRdd = sc.makeRDD(List(1,1,2,3,3,8)).subtract(sc.makeRDD(List(3,4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[Int] = Array(1, 1, 2, 8)\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subRdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartesian(otherRdd)\n",
    "```scala\n",
    "def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]           \n",
    "```\n",
    "笛卡尔积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cartesianRdd: org.apache.spark.rdd.RDD[(Int, String)] = CartesianRDD[39] at cartesian at <console>:25\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cartesianRdd = sc.makeRDD(List(1,1,2,3,3)).cartesian(sc.makeRDD(List(\"a\",\"b\",\"c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[(Int, String)] = Array((1,a), (1,b), (1,c), (1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c), (3,a), (3,b), (3,c))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesianRdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,a)\n",
      "(1,b)\n",
      "(1,c)\n",
      "(1,a)\n",
      "(1,b)\n",
      "(1,c)\n",
      "(2,a)\n",
      "(2,b)\n",
      "(2,c)\n",
      "(3,a)\n",
      "(3,b)\n",
      "(3,c)\n",
      "(3,a)\n",
      "(3,b)\n",
      "(3,c)\n"
     ]
    }
   ],
   "source": [
    "cartesianRdd.collect.foreach{case (a,b) => println(a,b)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cartesianRdd: org.apache.spark.rdd.RDD[(Int, Int)] = CartesianRDD[42] at cartesian at <console>:25\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cartesianRdd = sc.makeRDD(List(1,1,2,3,3)).cartesian(sc.makeRDD(List(21,22,23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[(Int, Int)] = Array((1,21), (1,22), (1,23), (1,21), (1,22), (1,23), (2,21), (2,22), (2,23), (3,21), (3,22), (3,23), (3,21), (3,22), (3,23))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesianRdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortBy（func，[ascending], [numTasks]）\n",
    "```scala\n",
    "  //Return this RDD sorted by the given key function.\n",
    "  def sortBy[K](\n",
    "      f: (T) => K,\n",
    "      ascending: Boolean = true,\n",
    "      numPartitions: Int = this.partitions.length)\n",
    "      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]          \n",
    "```\n",
    "返回一个新的RDD，排序按照元素经过func函数计算后返回的key比较。（默认方式为false，升序；true是降序）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[43] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(x => x).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[Int] = Array(1, 10, 2, 3, 4, 5, 6, 7, 8, 9)\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(x => x.toString).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[Int] = Array(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(x => x,false).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions(func)\n",
    "```scala\n",
    "  //Return a new RDD by applying a function to each partition of this RDD.\n",
    "  def mapPartitions[U: ClassTag](\n",
    "      f: Iterator[T] => Iterator[U],\n",
    "      preservesPartitioning: Boolean = false): RDD[U]      \n",
    "```\n",
    "针对每个分区进行操作，func要求传入一个Iterator，并且返回一个Iterator。使用 mapPartitions 函数获得输入 RDD 的每个分区中的元素迭代器，而需要返回的是执行结果的序列的迭代器。\n",
    "\n",
    "类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] => Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区\n",
    "\n",
    "preservesPartitioning表示返回RDD是否留有分区器。仅当RDD为K-V型RDD，且key没有被修饰的情况下，可设为true。非K-V型RDD一般不存在分区器；K-V RDD key被修改后，元素将不再满足分区器的分区要求。这些情况下，须设为false，表示返回的RDD没有被分区器分过区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[59] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "func: Iterator[Int] => Iterator[Int] = <function1>\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val func = (iter:Iterator[Int]) =>{\n",
    "  var sum = 0\n",
    "  while(iter.hasNext){\n",
    "    sum += iter.next\n",
    "  }\n",
    "  List(sum).iterator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[Int] = Array(6, 15, 34, 36, 45, 74)\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions(func).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "func1: (iter: Iterator[Int])Iterator[Int]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func1(iter:Iterator[Int]):Iterator[Int] ={\n",
    "  var sum = 0\n",
    "  while(iter.hasNext){\n",
    "    sum += iter.next\n",
    "  }\n",
    "  List(sum).iterator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Array[Int] = Array(6, 15, 34, 36, 45, 74)\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions(func1).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Array[Int] = Array(6, 15, 34, 36, 45, 74)\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions{iter => List(iter.sum).iterator}.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[Int] = Array(3, 6, 10, 13, 16, 20)\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions{iter => List(iter.max).iterator}.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[64] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List((\"kpop\",\"female\"),(\"zorro\",\"male\"),(\"mobin\",\"male\"),(\"lucy\",\"female\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[(String, String)] = Array((kpop,female), (lucy,female))\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions{iter => iter.filter{case (_,v) => v == \"female\"}}.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitionsWithIndex(func)\n",
    "```scala\n",
    "  //Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.\n",
    "  def mapPartitionsWithIndex[U: ClassTag](\n",
    "      f: (Int, Iterator[T]) => Iterator[U],\n",
    "      preservesPartitioning: Boolean = false): RDD[U]      \n",
    "```\n",
    "类似于mapPartitions，但func带有一个整数参数表示分片(分区)的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) => Iterator[U]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[66] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Array[(Int, List[Int])] = Array((0,List(1, 2, 3)), (1,List(4, 5, 6)), (2,List(7, 8, 9, 10)), (3,List(11, 12, 13)), (4,List(14, 15, 16)), (5,List(17, 18, 19, 20)))\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex( (index,iter) => List((index,iter.toList)).toIterator ).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[68] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[(Int, List[Int])] = Array((0,List()), (1,List(1)), (2,List(2)), (3,List()), (4,List(3)), (5,List(4)))\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex( (index,iter) => List((index,iter.toList)).toIterator ).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，分区中元素为空的分区也调用了此函数，每个分区调用一次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample(withReplacement, fraction, [seed])\n",
    "```scala\n",
    "  //Return a sampled subset of this RDD.\n",
    "  def sample(withReplacement: Boolean,fraction: Double,seed: Long = Utils.random.nextLong): RDD[T]     \n",
    "```\n",
    "在RDD中以seed为种子返回大致上有fraction比例的数据样本的RDD，withReplacement表示是否采用有放回的抽样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: Array[Int] = Array(1, 16, 20, 22, 29, 30, 32, 38, 43, 46, 52, 56, 62, 71, 76, 87, 89, 91, 92, 95)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(false,0.2,3).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Array[Int] = Array(1, 16, 20, 22, 29, 30, 32, 38, 43, 46, 52, 56, 62, 71, 76, 87, 89, 91, 92, 95)\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(false,0.2,3).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Array[Int] = Array(7, 9, 10, 13, 23, 24, 29, 44, 46, 48, 50, 54, 55, 59, 66, 68, 74, 79, 81, 83, 86, 91, 95)\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(false,0.2).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Array[Int] = Array(3, 6, 11, 14, 16, 21, 27, 34, 43, 45, 52, 63, 65, 68, 76, 79, 82, 91, 95, 96)\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sample(false,0.2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition(numPartitions)\n",
    "```scala\n",
    "  //Return a new RDD that has exactly numPartitions partitions.\n",
    "  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {\n",
    "    coalesce(numPartitions, shuffle = true)\n",
    "  }   \n",
    "```\n",
    "根据分区数，从新通过网络随机洗牌所有数据。当指定的分区数比当前分区数目少时，考虑使用coalesce，这样能够避免shuffle。\n",
    "\n",
    "有时，我们希望在除分组操作和聚合操作之外的操作中也能改变RDD的分区。对于这样的情况，Spark提供了repartition()函数。它会把数据通过网络进行混洗，并创建出新的分区集合。切记，对数据进行重新分区是代价相对比较大的操作。Spark中也有一个优化版的repartition()，叫作coalesce()。你可以使用Java或Scala中的rdd.partitions.size()以及Python中的rdd.getNumPartitions查看RDD的分区数，并确保调用coalesce()时将RDD合并到比现在的分区数更少的分区中。\n",
    "\n",
    "Spark提供了两种方法来对操作的并行度进行调优。第一种方法是在数据混洗操作时，使用参数的方式为混洗后的RDD指定并行度。第二种方法是对于任何已有的RDD，可以进行重新分区来获取更多或者更少的分区数。重新分区操作通过repartition()实现，该操作会把RDD随机打乱并分成设定的分区数目。如果你确定要减少RDD分区，可以使用coalesce()操作。由于没有打乱数据，该操作比repartition()更为高效。如果你认为当前的并行度过高或者过低，可以利用这些方法对数据分布进行重新调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[75] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 20,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Int = 6\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.partitions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[79] at repartition at <console>:27\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rerdd = rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Int = 10\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerdd.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce(numPartitions)\n",
    "```scala\n",
    "  def coalesce(numPartitions: Int, shuffle: Boolean = false,\n",
    "               partitionCoalescer: Option[PartitionCoalescer] = Option.empty)\n",
    "              (implicit ord: Ordering[T] = null)\n",
    "      : RDD[T]   \n",
    "```\n",
    "缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。\n",
    "\n",
    "重新给RDD的元素分区。  \n",
    "当适当缩小分区数时，如1000->100，spark会把之前的10个分区当作一个分区，并行度变为100，不会引起数据shuffle。  \n",
    "当严重缩小分区数时，如1000->1，运算时的并行度会变成1。为了避免并行效率低下问题，可将shuffle设为true。shuffle之前的运算和之后的运算分为不同stage，它们的并行度分别为1000,1。  \n",
    "当把分区数增大时，必会存在shuffle，shuffle须设为true。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[80] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 20,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rerdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[81] at coalesce at <console>:27\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rerdd = rdd.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Int = 2\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerdd.partitions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glom\n",
    "```scala\n",
    "def glom(): RDD[Array[T]]  \n",
    "```\n",
    "将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[82] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: Array[(Int, List[Int])] = Array((0,List(1, 2, 3)), (1,List(4, 5, 6)), (2,List(7, 8, 9, 10)), (3,List(11, 12, 13)), (4,List(14, 15, 16)), (5,List(17, 18, 19, 20)))\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex( (index,iter) => List((index,iter.toList)).toIterator ).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res36: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9, 10), Array(11, 12, 13), Array(14, 15, 16), Array(17, 18, 19, 20))\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipe(command, [envVars])\n",
    "```scala\n",
    "def pipe(command: String): RDD[String]\n",
    "def pipe(command: String, env: Map[String, String]): RDD[String] \n",
    "```\n",
    "对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD\n",
    "\n",
    "管道(pipe)操作：\n",
    "\n",
    "spark在RDD上提供了 pipe() 方法。通过pipe()，你可以使用任意语言将RDD中的各元素从标准输入流中以字符串形式读出，并将这些元素执行任何你需要的操作，然后把结果以字符串形式写入标准输出，这个过程就是RDD的转化操作过程。\n",
    "\n",
    "使用pipe()的方法很简单，假如我们有一个用其他语言写成的从标准输入接收数据并将处理结果写入标准输出的可执行脚本，我们只需要将该脚本分发到各个节点相同路径下，并将其路径作为pipe()的参数传入即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个例子：\n",
    "\n",
    "**注意：shell脚本需要集群中的所有节点都能访问到。**\n",
    "```scala\n",
    "scala> val rdd = sc.parallelize(List(\"hi\",\"Hello\",\"how\",\"are\",\"you\"),1)\n",
    "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at <console>:24\n",
    "\n",
    "scala> rdd.pipe(\"/home/bigdata/pipe.sh\").collect()\n",
    "res18: Array[String] = Array(AA, >>>hi, >>>Hello, >>>how, >>>are, >>>you)\n",
    "\n",
    "scala> val rdd = sc.parallelize(List(\"hi\",\"Hello\",\"how\",\"are\",\"you\"),2)\n",
    "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at <console>:24\n",
    "\n",
    "scala> rdd.pipe(\"/home/bigdata/pipe.sh\").collect()\n",
    "res19: Array[String] = Array(AA, >>>hi, >>>Hello, AA, >>>how, >>>are, >>>you)\n",
    "\n",
    "pipe.sh:\n",
    "#!/bin/sh\n",
    "echo \"AA\"\n",
    "while read LINE; do\n",
    "   echo \">>>\"${LINE}\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDD的转化操作\n",
    "Spark为包含键值对类型的RDD提供了一些专有的操作。这些RDD被称为pair RDD1。PairRDD是很多程序的构成要素，因为它们提供了并行操作各个键或跨节点重新进行数据分组的操作接口。例如，pairRDD提供reduceByKey()方法，可以分别归约每个键对应的数据，还有join()方法，可以把两个RDD中键相同的元素组合到一起，合并为一个RDD。我们通常从一个RDD中提取某些字段（例如代表事件时间、用户ID或者其他标识符的字段），并使用这些字段作为pair RDD操作中的键。\n",
    "\n",
    "在Spark中有很多种创建pair RDD的方式。此处不讲，很多存储键值对的数据格式会在读取时直接返回由其键值对数据组成的pairRDD。此外，当需要把一个普通的RDD转为pairRDD时，可以调用map()函数来实现，传递的函数需要返回键值对。\n",
    "\n",
    "在Scala中，为了让提取键之后的数据能够在函数中使用，同样需要返回二元组。隐式转换可以让二元组RDD支持附加的键值对函数。\n",
    "```\n",
    "//在Scala中使用第一个单词作为键创建出一个pair RDD\n",
    "val pairs = lines.map(x => (x.split(\" \")(0), x))\n",
    "```\n",
    "\n",
    "Pair RDD 可以使用所有标准 RDD 上的可用的转化操作。由于 pair RDD 中包含二元组，所以需要传递的函数应当操作二元组而不是独立的元素。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keys()\n",
    "```scala\n",
    "//Return an RDD with the keys of each tuple.\n",
    "def keys: RDD[K] = self.map(_._1)  \n",
    "```\n",
    "返回一个仅包含键的 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[85] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List( (\"tom\", 1), (\"jerry\", 2), (\"kitty\", 3),(\"jerry\", 9), (\"tom\", 8), (\"shuke\", 7) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: Array[String] = Array(tom, jerry, kitty, jerry, tom, shuke)\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### values()\n",
    "```scala\n",
    "//Return an RDD with the values of each tuple.\n",
    "def values: RDD[V] = self.map(_._2)\n",
    "```\n",
    "返回一个仅包含值的 RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Array[Int] = Array(1, 2, 3, 9, 8, 7)\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.values.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapValues(func)\n",
    "```scala\n",
    "def mapValues[U](f: V => U): RDD[(K, U)]\n",
    "```\n",
    "对 pair RDD 中的每个值应用一个函数而不改变键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: Array[(String, Int)] = Array((tom,10), (jerry,20), (kitty,30), (jerry,90), (tom,80), (shuke,70))\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(_ * 10).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMapValues(func)\n",
    "```scala\n",
    "def flatMapValues[U](f: V => TraversableOnce[U]): RDD[(K, U)]\n",
    "```\n",
    "对 pair RDD 中的每个值应用一个返回迭代器的函数， 然后对返回的每个元素都生成一个对应原键的键值对记录。 通常用于符号化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Array[(String, Int)] = Array((tom,1), (tom,1), (jerry,2), (jerry,2), (kitty,3), (kitty,3), (jerry,9), (jerry,9), (tom,8), (tom,8), (shuke,7), (shuke,7))\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMapValues( v => List(v,v)).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partitionBy(partitioner)\n",
    "```scala\n",
    "//Return a copy of the RDD partitioned using the specified partitioner.\n",
    "def partitionBy(partitioner: Partitioner): RDD[(K, V)]   \n",
    "```\n",
    "该函数根据partitioner函数生成新的ShuffleRDD，将原RDD重新分区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[90] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List( (\"a\",1),(\"b\",1),(\"c\",1),(\"d\",1),(\"a\",1),(\"b\",1),(\"g\",1),(\"c\",1),(\"b\",1),(\"g\",1),(\"d\",1) ) ,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: Array[(Int, List[(String, Int)])] = Array((0,List((a,1), (b,1))), (1,List((c,1), (d,1), (a,1))), (2,List((b,1), (g,1), (c,1))), (3,List((b,1), (g,1), (d,1))))\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex( (index,iter) => List((index,iter.toList)).toIterator ).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[92] at partitionBy at <console>:27\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = rdd.partitionBy(new org.apache.spark.HashPartitioner(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Array[(Int, List[(String, Int)])] = Array((0,List((d,1), (d,1))), (1,List((a,1), (a,1))), (2,List((b,1), (b,1), (b,1))), (3,List((c,1), (g,1), (c,1), (g,1))))\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.mapPartitionsWithIndex( (index,iter) => List((index,iter.toList)).toIterator ).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey(func, [numTasks])\n",
    "```scala\n",
    "  def reduceByKey(func: (V, V) => V): RDD[(K, V)] = self.withScope {\n",
    "    reduceByKey(defaultPartitioner(self), func)\n",
    "  }\n",
    "\n",
    "  def reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)] = self.withScope {\n",
    "    reduceByKey(new HashPartitioner(numPartitions), func)\n",
    "  }\n",
    "\n",
    "  def reduceByKey(partitioner: Partitioner, func: (V, V) => V): RDD[(K, V)] = self.withScope {\n",
    "    combineByKeyWithClassTag[V]((v: V) => v, func, func, partitioner)\n",
    "  }\n",
    "```\n",
    "在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[94] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List( (\"tom\", 1), (\"jerry\", 2), (\"kitty\", 3),(\"jerry\", 9), (\"tom\", 8), (\"shuke\", 7) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Array[(String, Int)] = Array((tom,9), (kitty,3), (jerry,11), (shuke,7))\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(_+_).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reduceByKey](imgs/reduceByKey.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res44: Array[(String, Int)] = Array((tom,9), (kitty,3), (jerry,11), (shuke,7))\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey( (x,y) => x + y  ).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey（[numTasks]）\n",
    "```scala\n",
    "  def groupByKey(): RDD[(K, Iterable[V])] = self.withScope {\n",
    "    groupByKey(defaultPartitioner(self))\n",
    "  }\n",
    "\n",
    "  def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] = self.withScope {\n",
    "    groupByKey(new HashPartitioner(numPartitions))\n",
    "  }\n",
    "\n",
    "    def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]\n",
    "```\n",
    "在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[97] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List( (\"tom\", 1), (\"jerry\", 2), (\"kitty\", 3),(\"jerry\", 9), (\"tom\", 8), (\"shuke\", 7) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "groupRdd: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[98] at groupByKey at <console>:27\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupRdd = rdd.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Array[(String, Iterable[Int])] = Array((tom,CompactBuffer(1, 8)), (kitty,CompactBuffer(3)), (jerry,CompactBuffer(2, 9)), (shuke,CompactBuffer(7)))\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupRdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: Array[(String, Int)] = Array((tom,9), (kitty,3), (jerry,11), (shuke,7))\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupRdd.map{ case (name,values) => (name,values.sum) }.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy（[numTasks]）\n",
    "```scala\n",
    "  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {\n",
    "    groupBy[K](f, defaultPartitioner(this))\n",
    "  }\n",
    "\n",
    "  def groupBy[K](\n",
    "      f: T => K,\n",
    "      numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {\n",
    "    groupBy(f, new HashPartitioner(numPartitions))\n",
    "  }\n",
    "\n",
    "  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)\n",
    "      : RDD[(K, Iterable[T])] = withScope {\n",
    "    val cleanF = sc.clean(f)\n",
    "    this.map(t => (cleanF(t), t)).groupByKey(p)\n",
    "  }\n",
    "```\n",
    "传入一个参数的函数，按照传入的参数为key，返回一个新的RDD[(K, Iterable[T])]，value是所有可以相同的传入数据组成的迭代器。**这是普通RDD的函数**\n",
    "\n",
    "这个函数可以把一个普通的RDD转成了Pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int, Int)] = ParallelCollectionRDD[100] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd=sc.parallelize(List((\"a\",1,2),(\"b\",1,1),(\"a\",4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Array[(String, Iterable[(String, Int, Int)])] = Array((a,CompactBuffer((a,1,2), (a,4,5))), (b,CompactBuffer((b,1,1))))\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupBy(_._1).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实Pair RDD也可以使用这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[103] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairRdd = sc.parallelize( List( (\"tom\", 1), (\"jerry\", 2), (\"kitty\", 3),(\"jerry\", 9), (\"tom\", 8), (\"shuke\", 7) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: Array[(String, Iterable[(String, Int)])] = Array((tom,CompactBuffer((tom,1), (tom,8))), (kitty,CompactBuffer((kitty,3))), (jerry,CompactBuffer((jerry,2), (jerry,9))), (shuke,CompactBuffer((shuke,7))))\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.groupBy(_._1).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey[C](  createCombiner: V => C,  mergeValue: (C, V) => C,  mergeCombiners: (C, C) => C) \n",
    "\n",
    "```scala\n",
    "  def combineByKey[C](\n",
    "      createCombiner: V => C,\n",
    "      mergeValue: (C, V) => C,\n",
    "      mergeCombiners: (C, C) => C): RDD[(K, C)] = self.withScope {\n",
    "    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null)\n",
    "  }\n",
    "\n",
    "  def combineByKey[C](\n",
    "      createCombiner: V => C,\n",
    "      mergeValue: (C, V) => C,\n",
    "      mergeCombiners: (C, C) => C,\n",
    "      numPartitions: Int): RDD[(K, C)] = self.withScope {\n",
    "    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners, numPartitions)(null)\n",
    "  }\n",
    "\n",
    "  def combineByKey[C](\n",
    "      createCombiner: V => C,\n",
    "      mergeValue: (C, V) => C,\n",
    "      mergeCombiners: (C, C) => C,\n",
    "      partitioner: Partitioner,\n",
    "      mapSideCombine: Boolean = true,\n",
    "      serializer: Serializer = null): RDD[(K, C)] = self.withScope {\n",
    "    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners,\n",
    "      partitioner, mapSideCombine, serializer)(null)\n",
    "  }\n",
    "```\n",
    "和reduceByKey的效果相同，reduceByKey底层就是调用combineByKey\n",
    "\n",
    "根据key分别使用createCombiner和mergeValue进行相同key的数值聚集，通过mergeCombiners将各个分区最终的结果进行聚集。\n",
    "\n",
    "参数说明：  \n",
    "* 第一个参数createCombiner: V => C：生成合并器，每组key，取出第一个value的值，然后返回你想合并的类型。  \n",
    "* 第二个参数mergeValue: (C, V) => C：函数，局部计算  \n",
    "* 第三个参数mergeCombiners: (C, C) => C：函数，对局部计算的结果再进行计算  \n",
    "\n",
    "参数说明2： \n",
    "* 对相同K，把V合并成一个集合.  \n",
    "* createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值  \n",
    "* mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并  \n",
    "* mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[106] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//使用combineByKey()求每个键对应的平均值\n",
    "val rdd = sc.parallelize( List( (\"coffee\", 1), (\"coffee\", 2), (\"panda\", 3),(\"coffee\", 9) ) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[108] at map at <console>:31\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = rdd.combineByKey(\n",
    "(v) => (v, 1),\n",
    "(acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),\n",
    "(acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    ").map{ case (key, value) => (key, value._1 / value._2.toFloat) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: Array[(String, Float)] = Array((coffee,4.0), (panda,3.0))\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(coffee,4.0)\n",
      "(panda,3.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res50: Iterable[Unit] = ArrayBuffer((), ())\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collectAsMap().map(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: scala.collection.Map[String,Float] = Map(coffee -> 4.0, panda -> 3.0)\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combineByKey流程示意图：\n",
    "![combineByKey](imgs/combineByKey.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,combOp: (U, U) => U): RDD[(K, U)]\n",
    "\n",
    "```scala\n",
    "  def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,\n",
    "      combOp: (U, U) => U): RDD[(K, U)] = self.withScope {\n",
    "    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)\n",
    "  }\n",
    "\n",
    "  def aggregateByKey[U: ClassTag](zeroValue: U, numPartitions: Int)(seqOp: (U, V) => U,\n",
    "      combOp: (U, U) => U): RDD[(K, U)] = self.withScope {\n",
    "    aggregateByKey(zeroValue, new HashPartitioner(numPartitions))(seqOp, combOp)\n",
    "  }\n",
    "\n",
    "  def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) => U,\n",
    "      combOp: (U, U) => U): RDD[(K, U)] = self.withScope {\n",
    "    // Serialize the zero value to a byte array so that we can get a new clone of it on each key\n",
    "    val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue)\n",
    "    val zeroArray = new Array[Byte](zeroBuffer.limit)\n",
    "    zeroBuffer.get(zeroArray)\n",
    "\n",
    "    lazy val cachedSerializer = SparkEnv.get.serializer.newInstance()\n",
    "    val createZero = () => cachedSerializer.deserialize[U](ByteBuffer.wrap(zeroArray))\n",
    "\n",
    "    // We will clean the combiner closure later in `combineByKey`\n",
    "    val cleanedSeqOp = self.context.clean(seqOp)\n",
    "    combineByKeyWithClassTag[U]((v: V) => cleanedSeqOp(createZero(), v),\n",
    "      cleanedSeqOp, combOp, partitioner)\n",
    "  }\n",
    "```\n",
    "从aggregateByKey的源代码中，可以看出\n",
    "\n",
    "a.aggregateByKey把类型为(K,V)的RDD转换为类型为(K,U)的RDD，V和U的类型可以不一样，这一点跟combineByKey是一样的，即返回的二元组的值类型可以不一样\n",
    "\n",
    "b.aggregateByKey内部是通过调用combineByKey实现的，combineByKey的createCombiner函数逻辑由zeroValue这个变量实现，zeroValue作为聚合的初始值，通常对于加法聚合则为0，乘法聚合则为1，集合操作则为空集合\n",
    "\n",
    "c.seqOp在combineByKey中的功能是mergeValues，(U,V)=>U\n",
    "\n",
    "d.combOp在combineByKey中的功能是mergeCombiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[109] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//aggregateByKey()求每个键对应的平均值\n",
    "val rdd = sc.parallelize( List( (\"coffee\", 1), (\"coffee\", 2), (\"panda\", 3),(\"coffee\", 9) ) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[111] at map at <console>:30\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = rdd.aggregateByKey( (0,0) ) (\n",
    "(acc, v) => (acc._1 + v, acc._2 + 1),\n",
    "(acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    ").map{ case (key, value) => (key, value._1 / value._2.toFloat) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res52: scala.collection.Map[String,Float] = Map(coffee -> 4.0, panda -> 3.0)\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从求均值的实现来看，aggregate通过提供零值的方式，避免了combineByKey中的createCombiner步骤(createCombiner本质工作就是遇到第一个key时进行初始化操作，这个初始化不是提供零值，而是对第一个(k,v)进行转换得到c的初始值））"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下面的例子来看看aggregateByKey的计算流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[112] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//定义RDD\n",
    "val pairRDD = sc.parallelize(List( (\"cat\",2), (\"cat\", 5), (\"mouse\", 4),(\"cat\", 12), (\"dog\", 12), (\"mouse\", 2)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: Array[String] = Array([partID:0, val: (cat,2)], [partID:0, val: (cat,5)], [partID:0, val: (mouse,4)], [partID:1, val: (cat,12)], [partID:1, val: (dog,12)], [partID:1, val: (mouse,2)])\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//查看分区情况\n",
    "pairRDD.mapPartitionsWithIndex( \n",
    "    (index,iter) => iter.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\")\n",
    ").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Array[String] = Array(0:[(cat,2); (cat,5); (mouse,4)], 1:[(cat,12); (dog,12); (mouse,2)])\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.mapPartitionsWithIndex( \n",
    "    (index,iter) => List(index + \":\" + iter.mkString(\"[\",\"; \",\"]\")).iterator\n",
    ").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//注意：初始值为0和其他值的区别\n",
    "pairRDD.aggregateByKey(0)(_+_,_+_).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res56: Array[(String, Int)] = Array((dog,22), (cat,39), (mouse,26))\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.aggregateByKey(10)(_+_,_+_).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res57: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//下面三个的区别：，第一个比较好理解，由于初始值为0，所以每个分区输出不同动物中个数最多的那个，然后在累加\n",
    "pairRDD.aggregateByKey(0)(math.max(_,_),_+_).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res58: Array[(String, Int)] = Array((dog,12), (cat,22), (mouse,20))\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//下面两个：由于有初始值，就需要考虑初始值参与计算，这里第一个分区的元素为(\"cat\",2), (\"cat\", 5), (\"mouse\", 4)，初始值是10，不同动物之间两两比较value的大小，都需要将初始值加入比较，所以第一个分区输出为(\"cat\", 10), (\"mouse\", 10)；第二个分区同第一个分区，输出结果为(dog,12), (cat,12), (mouse,10)；所以最后累加的结果为(dog,12), (cat,22), (mouse,20)，注意最后的对每个分区结果计算的时候，初始值不参与计算\n",
    "pairRDD.aggregateByKey(10)(math.max(_,_),_+_).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: Array[(String, Int)] = Array((dog,100), (cat,200), (mouse,200))\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//这个和上面的类似\n",
    "pairRDD.aggregateByKey(100)(math.max(_,_),_+_).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]\n",
    "```scala\n",
    "  def foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)] = self.withScope {\n",
    "    foldByKey(zeroValue, defaultPartitioner(self))(func)\n",
    "  }\n",
    "\n",
    "  def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) => V): RDD[(K, V)] = self.withScope {\n",
    "    foldByKey(zeroValue, new HashPartitioner(numPartitions))(func)\n",
    "  }\n",
    "\n",
    "  def foldByKey(zeroValue: V,partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V)] = self.withScope {\n",
    "    ...\n",
    "    combineByKeyWithClassTag[V]((v: V) => cleanedFunc(createZero(), v),\n",
    "      cleanedFunc, cleanedFunc, partitioner)\n",
    "  }\n",
    "```\n",
    "aggregateByKey的简化操作，seqop和combop相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[120] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List( (\"coffee\", 1), (\"coffee\", 2), (\"panda\", 3),(\"coffee\", 9) ) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res60: Array[(String, Int)] = Array((coffee,12), (panda,3))\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.foldByKey(0)(_ + _).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey([ascending], [numTasks])\n",
    "```scala\n",
    "def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]\n",
    "```\n",
    "在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[122] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Array((3,\"aa\"),(6,\"cc\"),(2,\"bb\"),(1,\"dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: Array[(Int, String)] = Array((1,dd), (2,bb), (3,aa), (6,cc))\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey().collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: Array[(Int, String)] = Array((6,cc), (3,aa), (2,bb), (1,dd))\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey(false).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortBy(func,[ascending], [numTasks])\n",
    "```scala\n",
    "  def sortBy[K](\n",
    "      f: (T) => K,\n",
    "      ascending: Boolean = true,\n",
    "      numPartitions: Int = this.partitions.length)\n",
    "      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope {\n",
    "    this.keyBy[K](f)\n",
    "        .sortByKey(ascending, numPartitions)\n",
    "        .values\n",
    "  }\n",
    "```\n",
    "与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。\n",
    "\n",
    "底层实现还是sortByKey，使用func生成的key进行排序。**这是普通RDD的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[129] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,3,4,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: Array[Int] = Array(1, 2, 3, 4, 11)\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy( x => x ).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res64: Array[Int] = Array(1, 11, 2, 3, 4)\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy( _.toString ).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtractByKey(otherRdd)\n",
    "```scala\n",
    "def subtractByKey[W: ClassTag](other: RDD[(K, W)]): RDD[(K, V)] \n",
    "```\n",
    "删掉RDD中键与other RDD中的键相同的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[140] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List( (1, 1), (2, 2), (2, 3),(3, 9),(3, 9) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "otherRdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[141] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val otherRdd = sc.parallelize( Array( (3,\"a\"),(4,\"b\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res65: Array[(Int, Int)] = Array((1,1), (2,2), (2,3))\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtractByKey(otherRdd).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join(otherDataset, [numTasks])\n",
    "```scala\n",
    "  def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] = self.withScope {\n",
    "    join(other, defaultPartitioner(self, other))\n",
    "  }\n",
    "\n",
    "  def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))] = self.withScope {\n",
    "    join(other, new HashPartitioner(numPartitions))\n",
    "  }\n",
    "\n",
    "  def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope {\n",
    "    this.cogroup(other, partitioner).flatMapValues( pair =>\n",
    "      for (v <- pair._1.iterator; w <- pair._2.iterator) yield (v, w)\n",
    "    )\n",
    "  }\n",
    "```\n",
    "对两个RDD进行内连接，在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[143] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val left = sc.parallelize(List((1,\"a\"),(2,\"a\"),(2,\"b\"),(3,\"c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "right: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[144] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = sc.parallelize(Array((2,4),(3,5),(4,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Array[(Int, (String, Int))] = Array((2,(a,4)), (2,(b,4)), (3,(c,5)))\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.join(right).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,c,5)\n",
      "(2,a,4)\n",
      "(2,b,4)\n"
     ]
    }
   ],
   "source": [
    "left.join(right).foreach{ case (a,(b,c)) => println(a,b,c)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### leftOuterJoin(otherDataset, [numTasks])\n",
    "```scala\n",
    "  def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))] = self.withScope {\n",
    "    leftOuterJoin(other, defaultPartitioner(self, other))\n",
    "  }\n",
    "```\n",
    "左外连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,a,None)\n",
      "(3,c,Some(5))\n",
      "(2,a,Some(4))\n",
      "(2,b,Some(4))\n"
     ]
    }
   ],
   "source": [
    "left.leftOuterJoin(right).foreach{ case (a,(b,c)) => println(a,b,c)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rightOuterJoin(otherDataset, [numTasks])\n",
    "```scala\n",
    "  def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))] = self.withScope {\n",
    "    rightOuterJoin(other, defaultPartitioner(self, other))\n",
    "  }\n",
    "```\n",
    "右外连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,None,6)\n",
      "(3,Some(c),5)\n",
      "(2,Some(a),4)\n",
      "(2,Some(b),4)\n"
     ]
    }
   ],
   "source": [
    "left.rightOuterJoin(right).foreach{ case (a,(b,c)) => println(a,b,c)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cogroup(otherDataset, [numTasks])\n",
    "```scala\n",
    "  def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope {\n",
    "    cogroup(other, defaultPartitioner(self, other))\n",
    "  }\n",
    "```\n",
    "将两个RDD中拥有相同键的数据分组到一起。在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer())), (2,(CompactBuffer(a, b),CompactBuffer(4))), (3,(CompactBuffer(c),CompactBuffer(5))), (4,(CompactBuffer(),CompactBuffer(6))))\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.cogroup(right).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,a,4)\n",
      "(1,a,0)\n",
      "(4,null,6)\n",
      "(3,c,5)\n",
      "(2,b,0)\n"
     ]
    }
   ],
   "source": [
    "left.cogroup(right).foreach{ case (a,(b,c)) => b.zipAll(c,null,0).foreach{ case (b,c) => println(a,b,c) } }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个操作并没有实现真正的外联查询，应该改zipAll为一个实现了一个笛卡尔积的函数，可惜我没找到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,Some(a),None)\n",
      "(4,None,Some(6))\n",
      "(3,Some(c),Some(5))\n",
      "(2,Some(a),Some(4))\n",
      "(2,Some(b),Some(4))\n"
     ]
    }
   ],
   "source": [
    "left.fullOuterJoin(right).foreach{ case (a,(b,c)) => println(a,b,c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,c,5)\n",
      "(2,a,4)\n",
      "(2,b,4)\n"
     ]
    }
   ],
   "source": [
    "left.join(right).foreach{ case (a,(b,c)) => println(a,b,c)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zipAll函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: List[(Int, String)] = List((1,a), (2,b), (3,\"\"))\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1, 2, 3).zipAll(List(\"a\",\"b\"),0,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res75: List[(Int, String)] = List((1,a), (2,b), (0,c))\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1, 2).zipAll(List(\"a\",\"b\",\"c\"),0,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action\n",
    "一旦触发，就会执行一个任务\n",
    "\n",
    "RDD的一些行动操作会以普通集合或者值的形式将RDD的部分或全部数据返回驱动器程序中。\n",
    "\n",
    "把数据返回驱动器程序中最简单、最常见的操作是collect()，它会将整个RDD的内容返回。collect()通常在单元测试中使用，因为此时RDD的整个内容不会很大，可以放在内存中。使用collect()使得RDD的值与预期结果之间的对比变得很容易。由于需要将数据复制到驱动器进程中，collect()要求所有数据都必须能一同放入单台机器的内存中。\n",
    "\n",
    "take(n)返回RDD中的n个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合。需要注意的是，这些操作返回元素的顺序与你预期的可能不一样。\n",
    "\n",
    "这些操作对于单元测试和快速调试都很有用，但是在处理大规模数据时会遇到瓶颈。\n",
    "\n",
    "如果为数据定义了顺序，就可以使用top()从RDD中获取前几个元素。top()会使用数据的默认顺序，但我们也可以提供自己的比较函数，来提取前几个元素。\n",
    "\n",
    "有时需要在驱动器程序中对我们的数据进行采样。takeSample(withReplacement, num,seed)函数可以让我们从数据中获取一个采样，并指定是否替换。\n",
    "\n",
    "有时我们会对RDD中的所有元素应用一个行动操作，但是不把任何结果返回到驱动器程序中，这也是有用的。比如可以用JSON格式把数据发送到一个网络服务器上，或者把数据存到数据库中。不论哪种情况，都可以使用foreach()行动操作来对RDD中的每个元素进行操作，而不需要把RDD发回本地。\n",
    "\n",
    "关于基本RDD上的更多标准操作，我们都可以从其名称推测出它们的行为。count()用来返回元素的个数，而countByValue()则返回一个从各值到值对应的计数的映射表。\n",
    "\n",
    "### collect()\n",
    "```scala\n",
    "  //Return an array that contains all of the elements in this RDD.\n",
    "  def collect(): Array[T] = withScope {\n",
    "    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)\n",
    "    Array.concat(results: _*)\n",
    "  }\n",
    "```\n",
    "在驱动程序中，以数组的形式返回数据集的所有元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[167] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List(1, 2, 3, 3, 4 , 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res76: Array[Int] = Array(1, 2, 3, 3, 4, 5)\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()\n",
    "```scala\n",
    "  //Return the number of elements in the RDD.\n",
    "  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n",
    "```\n",
    "RDD中的元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res77: Long = 6\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByValue()\n",
    "```scala\n",
    "  //Return the count of each unique value in this RDD as a local map of (value, count) pairs.\n",
    "  //rdd.map(x => (x, 1L)).reduceByKey(_ + _)\n",
    "  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope {\n",
    "    map(value => (value, null)).countByKey()\n",
    "  }\n",
    "```\n",
    "各元素在RDD中出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[168] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize( List(1, 2, 3, 3, 4 , 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res78: scala.collection.Map[Int,Long] = Map(5 -> 1, 1 -> 1, 2 -> 1, 3 -> 2, 4 -> 1)\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey()\n",
    "```scala\n",
    "  def countByKey(): Map[K, Long] = self.withScope {\n",
    "    self.mapValues(_ => 1L).reduceByKey(_ + _).collect().toMap\n",
    "  }\n",
    "```\n",
    "对每个键对应的元素分别计数。**此函数Pair RDD的行动操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[172] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res79: scala.collection.Map[String,Long] = Map(a -> 1, b -> 2, c -> 2)\n"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res80: scala.collection.Map[(String, Int),Long] = Map((b,2) -> 2, (a,1) -> 1, (c,2) -> 1, (c,1) -> 1)\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collectAsMap()\n",
    "```scala\n",
    "  def collectAsMap(): Map[K, V] = self.withScope {\n",
    "    val data = self.collect()\n",
    "    val map = new mutable.HashMap[K, V]\n",
    "    map.sizeHint(data.length)\n",
    "    data.foreach { pair => map.put(pair._1, pair._2) }\n",
    "    map\n",
    "  }\n",
    "```\n",
    "将结果以映射表的形式返回，以便查询。**此函数Pair RDD的行动操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res81: scala.collection.Map[String,Int] = Map(dog -> 12, cat -> 12, mouse -> 2)\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.collectAsMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lookup(key)\n",
    "```scala\n",
    "  /**\n",
    "   * Return the list of values in the RDD for key `key`. This operation is done efficiently if the\n",
    "   * RDD has a known partitioner by only searching the partition that the key maps to.\n",
    "   */\n",
    "  def lookup(key: K): Seq[V] = self.withScope {\n",
    "    self.partitioner match {\n",
    "      case Some(p) =>\n",
    "        val index = p.getPartition(key)\n",
    "        val process = (it: Iterator[(K, V)]) => {\n",
    "          val buf = new ArrayBuffer[V]\n",
    "          for (pair <- it if pair._1 == key) {\n",
    "            buf += pair._2\n",
    "          }\n",
    "          buf\n",
    "        } : Seq[V]\n",
    "        val res = self.context.runJob(self, process, Array(index))\n",
    "        res(0)\n",
    "      case None =>\n",
    "        self.filter(_._1 == key).map(_._2).collect()\n",
    "    }\n",
    "  }\n",
    "```\n",
    "返回给定键对应的所有值。**此函数Pair RDD的行动操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[178] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res82: Seq[Int] = WrappedArray(2, 1)\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.lookup(\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(num)\n",
    "```scala\n",
    "def take(num: Int): Array[T]\n",
    "```\n",
    "从RDD中返回前num个元素的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[181] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(10, 4, 2, 12, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res83: Array[Int] = Array(10, 4)\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first()\n",
    "```scala\n",
    "  /**\n",
    "   * Return the first element in this RDD.\n",
    "   */\n",
    "  def first(): T = withScope {\n",
    "    take(1) match {\n",
    "      case Array(t) => t\n",
    "      case _ => throw new UnsupportedOperationException(\"empty collection\")\n",
    "    }\n",
    "  }\n",
    "```\n",
    "返回RDD的第一个元素（类似于take(1)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res84: Int = 10\n"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top(num)\n",
    "```scala\n",
    "  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {\n",
    "    takeOrdered(num)(ord.reverse)\n",
    "  }\n",
    "```\n",
    "top函数用于从RDD中，按照默认（降序）或者指定的排序规则，返回前num个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res85: Array[Int] = Array(12, 10)\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeOrdered(num)([Ordering])\n",
    "```scala\n",
    "def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]\n",
    "```\n",
    "从RDD中按照提供的顺序返回最前面的num个元素。takeOrdered和top类似，只不过以和top相反的顺序返回元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res86: Array[Int] = Array(2, 3)\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeOrdered(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample(withReplacement,num, [seed])\n",
    "```scala\n",
    "def takeSample(withReplacement: Boolean,num: Int,seed: Long = Utils.random.nextLong): Array[T]\n",
    "```\n",
    "返回一个数组，该数组由从数据集中随机采样的num个元素组成，seed用于指定随机数生成器种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res87: Array[Int] = Array(10, 4, 2, 12, 3)\n"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res88: Array[Int] = Array(12, 3, 10)\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(false,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res89: Array[Int] = Array(2, 3, 10, 4, 12)\n"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(false,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res90: Array[Int] = Array(4, 10, 4, 4, 2, 3, 10, 3, 3, 12)\n"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(true,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到有放回的抽样可以取出多于RDD中元素数量的元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce(func)\n",
    "```scala\n",
    "def reduce(f: (T, T) => T): T\n",
    "```\n",
    "通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。并行整合RDD中所有数据（例如sum）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res91: Int = 31\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce((x, y) => x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res92: Int = 31\n"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res93: Double = 31.0\n"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fold(num)(func)\n",
    "```scala\n",
    "def fold(zeroValue: T)(op: (T, T) => T): T\n",
    "```\n",
    "和reduce()一样，但是需要rdd.fold(0)((x, y) => x + y)提供初始值\n",
    "\n",
    "折叠操作，aggregate的简化操作，seqop和combop一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res94: Int = 31\n"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.fold(0)((x, y) => x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)\n",
    "```scala\n",
    "def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U\n",
    "```\n",
    "和reduce()相似，但是通常返回不同类型的函数\n",
    "\n",
    "aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。\n",
    "\n",
    "将RDD中元素聚集，须提供0初值（因为累积元素，所有要提供累积的初值）。先在分区内依照seqOp函数聚集元素（把T类型元素聚集为U类型的分区“结果”），再在分区间按照combOp函数聚集分区计算结果，最后返回这个结果\n",
    "\n",
    "第一个参数是初始值, 第二个参数:是两个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[187] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq(10, 4, 2, 12, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res95: (Int, Int) = (31,5)\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//总和sum以及count\n",
    "rdd.aggregate( (0, 0) ) (\n",
    "(x, y) =>(x._1 + y, x._2 + 1),\n",
    "(x, y) =>(x._1 + y._1, x._2 + y._2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1,1),4,   1=>   ,5,2)\n",
      "((1,1),2,   1=>   ,3,2)\n",
      "((1,1),3,   1=>   ,4,2)\n",
      "((1,1),12,   1=>   ,13,2)\n",
      "((1,1),10,   1=>   ,11,2)\n",
      "((1,1),(1,1),   2=>   ,2,2)\n",
      "((2,2),(3,2),   2=>   ,5,4)\n",
      "((5,4),(5,2),   2=>   ,10,6)\n",
      "((10,6),(4,2),   2=>   ,14,8)\n",
      "((14,8),(13,2),   2=>   ,27,10)\n",
      "((27,10),(11,2),   2=>   ,38,12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res96: (Int, Int) = (38,12)\n"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//看看aggregate的运行流程\n",
    "rdd.aggregate( (1, 1) ) (\n",
    "(x, y) =>{println(x,y,\"   1=>   \",x._1 + y, x._2 + 1);(x._1 + y, x._2 + 1)},\n",
    "(x, y) =>{println(x,y,\"   2=>   \",x._1 + y._1, x._2 + y._2);(x._1 + y._1, x._2 + y._2)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到zeroValue在每个分区内和分局之间的聚合都会计算，上面的分局之间的聚合多了一个(1,1)是因为，这个rdd有6个分区，其中一个为空。每个分区多加1，加上分局之间的聚合多加了一个1，所以初始值(1, 1)比(0, 0)结果多了7。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1,1),4,   1=>   ,5,2)\n",
      "((1,1),3,   1=>   ,4,2)\n",
      "((1,1),2,   1=>   ,3,2)\n",
      "((1,1),12,   1=>   ,13,2)\n",
      "((1,1),10,   1=>   ,11,2)\n",
      "((1,1),(5,2),   2=>   ,6,3)\n",
      "((6,3),(4,2),   2=>   ,10,5)\n",
      "((10,5),(3,2),   2=>   ,13,7)\n",
      "((13,7),(13,2),   2=>   ,26,9)\n",
      "((26,9),(11,2),   2=>   ,37,11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res97: (Int, Int) = (37,11)\n"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//看看5个分区的情况\n",
    "sc.parallelize(Seq(10, 4, 2, 12, 3) , 5)\n",
    ".aggregate( (1, 1) ) (\n",
    "(x, y) =>{println(x,y,\"   1=>   \",x._1 + y, x._2 + 1);(x._1 + y, x._2 + 1)},\n",
    "(x, y) =>{println(x,y,\"   2=>   \",x._1 + y._1, x._2 + y._2);(x._1 + y._1, x._2 + y._2)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1,1),4,   1=>   ,5,2)\n",
      "((5,2),2,   1=>   ,7,3)\n",
      "((1,1),12,   1=>   ,13,2)\n",
      "((13,2),3,   1=>   ,16,3)\n",
      "((1,1),10,   1=>   ,11,2)\n",
      "((1,1),(16,3),   2=>   ,17,4)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res98: (Int, Int) = (35,9)\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "((17,4),(7,3),   2=>   ,24,7)\n",
      "((24,7),(11,2),   2=>   ,35,9)\n"
     ]
    }
   ],
   "source": [
    "//看看3个分区的情况\n",
    "sc.parallelize(Seq(10, 4, 2, 12, 3) , 3)\n",
    ".aggregate( (1, 1) ) (\n",
    "(x, y) =>{println(x,y,\"   1=>   \",x._1 + y, x._2 + 1);(x._1 + y, x._2 + 1)},\n",
    "(x, y) =>{println(x,y,\"   2=>   \",x._1 + y._1, x._2 + y._2);(x._1 + y._1, x._2 + y._2)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再看下面的例子就是简简单单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[190] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res99: String = defabc\n"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//这里需要注意，由于每个分区计算是并行计算，所以计算出的结果有先后顺序，所以结果可能会出现两种情况：如下\n",
    "rdd2.aggregate(\"\")(_+_,_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res100: String = defabc\n"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//这里需要注意，由于每个分区计算是并行计算，所以计算出的结果有先后顺序，所以结果可能会出现两种情况：如下\n",
    "rdd2.aggregate(\"\")(_+_,_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res101: String = ==def=abc\n"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.aggregate(\"=\")(_+_,_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile(path)\n",
    "```scala\n",
    "def saveAsTextFile(path: String): Unit \n",
    "```\n",
    "将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。**path不能存在和hdfs类似**\n",
    "\n",
    "存到本地:\n",
    "```scala\n",
    "//存到了F:\\hadoop_test\\rdd目录下，每个分区一个文件和MapReduce操作类似，连文件名都差不多\n",
    "rdd.saveAsTextFile(\"\"\"F:\\hadoop_test\\rdd\"\"\")\n",
    "```\n",
    "![saveAsTextFile](imgs/saveAsTextFile.jpg)\n",
    "\n",
    "\n",
    "存到hdfs:\n",
    "```scala\n",
    "rdd.saveAsTextFile(\"\"\"hdfs://hadoop01:9000/out10\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsSequenceFile(path) \n",
    "```scala\n",
    "class SequenceFileRDDFunctions[K <% Writable: ClassTag, V <% Writable : ClassTag](\n",
    "    self: RDD[(K, V)],\n",
    "    _keyWritableClass: Class[_ <: Writable],\n",
    "    _valueWritableClass: Class[_ <: Writable])\n",
    "  extends Logging\n",
    "  with Serializable {\n",
    "  def saveAsSequenceFile(\n",
    "      path: String,\n",
    "      codec: Option[Class[_ <: CompressionCodec]] = None): Unit\n",
    "}\n",
    "```\n",
    "将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。**键值对的RDD支持这个函数**\n",
    "\n",
    "```scala\n",
    "pairRDD.saveAsSequenceFile(\"\"\"F:\\hadoop_test\\rdd1\"\"\")\n",
    "pairRDD.saveAsSequenceFile(\"\"\"hdfs://hadoop01:9000/out11\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsObjectFile(path) \n",
    "```scala\n",
    "//Save this RDD as a SequenceFile of serialized objects.\n",
    "//def saveAsObjectFile(path: String): Unit\n",
    "```\n",
    "用于将RDD中的元素序列化成对象，存储到文件中。\n",
    "\n",
    "```scala\n",
    "rdd..saveAsSequenceFile(\"\"\"F:\\hadoop_test\\rdd1\"\"\")\n",
    "rdd..saveAsSequenceFile(\"\"\"hdfs://hadoop01:9000/out11\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "//rdd.saveAsObjectFile(\"\"\"F:\\hadoop_test\\rdd1\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach(func)\n",
    "```scala\n",
    "  //Applies a function f to all elements of this RDD.\n",
    "  def foreach(f: T => Unit): Unit = withScope {\n",
    "    val cleanF = sc.clean(f)\n",
    "    sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))\n",
    "  }\n",
    "```\n",
    "\n",
    "在数据集的每一个元素上，运行func函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[193] at makeRDD at <console>:25\r\n",
       "sum: org.apache.spark.Accumulator[Int] = 0\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rdd = sc.makeRDD(1 to 10,2)\n",
    "var sum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(sum+=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res104: Int = 55\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
